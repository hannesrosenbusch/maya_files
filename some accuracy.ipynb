{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import pandas as pd\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(42)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "#define function to turn grayscale into rgb\n",
    "def to_rgb(im):\n",
    "    '''\n",
    "    # I think this will be slow\n",
    "    w, h = im.shape\n",
    "    ret = np.empty((w, h, 3), dtype=np.uint8)\n",
    "    ret[:, :, 0] = im\n",
    "    ret[:, :, 1] = im\n",
    "    ret[:, :, 2] = im\n",
    "    return ret\n",
    "    '''\n",
    "    # maya - this should run faster\n",
    "    #ret = cv2.cvtColor(im,cv2.COLOR_GRAY2RGB)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0_the gap666.jpg\n",
      "1    1_topshop1716.jpg\n",
      "2    10_topshop894.jpg\n",
      "Name: player.photo, dtype: object\n"
     ]
    }
   ],
   "source": [
    "target = 'sexual_interest'\n",
    "target = ['trustworthiness', \"sexual_interest\", \"intelligence\", \"confidence\",\"happiness\"]\n",
    "\n",
    "#set constants\n",
    "limit = 5000 #with how many pictures do I want to experiment\n",
    "# maya - I think you can choose smaller image size to start with. smaller the input, faster the process\n",
    "# I also think Resnet accepts various size inputs, you can try giving it actual size images without deformation\n",
    "height = 224\n",
    "width = 224\n",
    "cv_folds = 4\n",
    "batchsize = 32\n",
    "preprocess = False\n",
    "nr_epochs = 5\n",
    "rescaling = True #True False\n",
    "lrr = 0.001 #0.01 0.005 0.001 0.0005 0.0001\n",
    "unfreeze = 176 #-8 -5 -2 176\n",
    "\n",
    "#read labels\n",
    "labels = pd.read_csv('../input/labels/labels.csv', header = 'infer')\n",
    "print(labels['player.photo'][0:3])\n",
    "Y = np.array(labels[target][0:limit])\n",
    "labels['file'] = \"../input/resized-images/resized_images/resized_images/\" + labels['player.photo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if preprocess:\n",
    "    # read images\n",
    "    X = np.empty(shape=(limit, height, width, 3), dtype='uint8')\n",
    "    images = [np.array(Image.open(\"images/\" + fname)) for fname in labels['player.photo'][0:limit]]\n",
    "\n",
    "    #bring into same format\n",
    "    for i, image in enumerate(images):\n",
    "        if len(image.shape) == 2:\n",
    "            special_case = labels['player.photo'][i]\n",
    "            resized_img = cv2.imread(\"images/\" + special_case, cv2.IMREAD_GRAYSCALE)\n",
    "            resized_img = Image.fromarray(resized_img)\n",
    "            resized_img = resized_img.resize((224,224))\n",
    "            resized_img = np.array(resized_img)\n",
    "            resized_img = to_rgb(resized_img)\n",
    "            # plt.imshow(Image.fromarray(resized_img, mode= 'RGB'))\n",
    "            # plt.show()\n",
    "\n",
    "\n",
    "        elif image.shape[2] == 4 or image.shape[2] == 3:\n",
    "            resized_img = Image.fromarray(image[...,:3])\n",
    "            resized_img = resized_img.resize((height,width))\n",
    "            resized_img = np.array(resized_img)\n",
    "\n",
    "        elif image.shape[2] == 2:\n",
    "            image = np.array(image[...,0], dtype= 'uint8')\n",
    "            resized_img = Image.fromarray(image)\n",
    "            resized_img = resized_img.resize((224,224))\n",
    "            resized_img = np.array(resized_img)\n",
    "            resized_img = to_rgb(resized_img)\n",
    "\n",
    "        else:\n",
    "            print(\"There are other formats!\")\n",
    "            print(image.shape)\n",
    "            sys.exit()\n",
    "\n",
    "        X[i] = resized_img\n",
    "\n",
    "    np.save(r\"../input/preprocessed images/preprocessed_data.npy\", X)\n",
    "\n",
    "else:\n",
    "    X = np.load(r\"../input/preprocessed-images/preprocessed_data.npy\")\n",
    "    X = X[0:limit]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 3s 0us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 10245     \n",
      "=================================================================\n",
      "Total params: 23,597,957\n",
      "Trainable params: 10,245\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#load resnet50\n",
    "resnet = ResNet50(include_top=False, pooling='avg', weights = 'imagenet')\n",
    "for layer in resnet.layers[:unfreeze]:\n",
    "    layer.trainable = False\n",
    "model = Sequential()\n",
    "model.add(resnet)\n",
    "\n",
    "#add linear last layer\n",
    "model.add(Dense(5, activation='linear'))\n",
    "\n",
    "#compile model\n",
    "# maya - here I do propose to do a grid-search to find the best fine-tuning parameters\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=Adam(lr=lrr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),\n",
    "              metrics=['mse'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data augmentation\n",
    "if rescaling:\n",
    "    datagen = ImageDataGenerator(rescale = 1./255.,\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            zoom_range=0.25, horizontal_flip=True, vertical_flip=False)\n",
    "if not rescaling:\n",
    "        datagen = ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            zoom_range=0.25, horizontal_flip=True, vertical_flip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code is to check whether the pictures in X and in the labels dataframe are in the same order (important to avoid information leak below)\n",
    "\n",
    "# sample = 245\n",
    "# sample_array = np.array([sample])\n",
    "# for x, y in datagen.flow_from_dataframe(dataframe=labels.iloc[sample_array], directory=\"../input/resized-images/resized_images/resized_images/\", \n",
    "#                                                   x_col=\"player.photo\", y_col=\"sexual_interest\", has_ext=True, \n",
    "#                                                   class_mode=\"other\", target_size=(224, 224), \n",
    "#                                                   batch_size=1):\n",
    "#     test = np.array(x[0], dtype = 'uint8')\n",
    "#     plt.imshow(Image.fromarray(test, mode= 'RGB'))\n",
    "#     plt.show()\n",
    "#     break\n",
    "\n",
    "# test = np.array(X[sample], dtype = 'uint8')\n",
    "# plt.imshow(Image.fromarray(test, mode= 'RGB'))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3750 validated image filenames.\n",
      "Epoch 1/5\n",
      "118/117 [==============================] - 48s 408ms/step - loss: 2.8882 - mean_squared_error: 2.8882 - val_loss: 3.6240 - val_mean_squared_error: 3.6240\n",
      "Epoch 2/5\n",
      "118/117 [==============================] - 40s 340ms/step - loss: 1.0752 - mean_squared_error: 1.0752 - val_loss: 3.3219 - val_mean_squared_error: 3.3219\n",
      "Epoch 3/5\n",
      "118/117 [==============================] - 41s 346ms/step - loss: 0.8932 - mean_squared_error: 0.8932 - val_loss: 2.4584 - val_mean_squared_error: 2.4584\n",
      "Epoch 4/5\n",
      "118/117 [==============================] - 41s 346ms/step - loss: 0.7618 - mean_squared_error: 0.7618 - val_loss: 2.0716 - val_mean_squared_error: 2.0716\n",
      "Epoch 5/5\n",
      "118/117 [==============================] - 41s 346ms/step - loss: 0.6929 - mean_squared_error: 0.6929 - val_loss: 1.8040 - val_mean_squared_error: 1.8040\n",
      "Found 3750 validated image filenames.\n",
      "Epoch 1/5\n",
      "118/117 [==============================] - 44s 374ms/step - loss: 0.6625 - mean_squared_error: 0.6625 - val_loss: 1.8904 - val_mean_squared_error: 1.8904\n",
      "Epoch 2/5\n",
      "118/117 [==============================] - 41s 350ms/step - loss: 0.6352 - mean_squared_error: 0.6352 - val_loss: 1.9832 - val_mean_squared_error: 1.9832\n",
      "Epoch 3/5\n",
      "118/117 [==============================] - 41s 347ms/step - loss: 0.6085 - mean_squared_error: 0.6085 - val_loss: 1.7625 - val_mean_squared_error: 1.7625\n",
      "Epoch 4/5\n",
      "118/117 [==============================] - 41s 347ms/step - loss: 0.5792 - mean_squared_error: 0.5792 - val_loss: 1.4369 - val_mean_squared_error: 1.4369\n",
      "Epoch 5/5\n",
      "118/117 [==============================] - 41s 348ms/step - loss: 0.5642 - mean_squared_error: 0.5642 - val_loss: 1.9664 - val_mean_squared_error: 1.9664\n",
      "Found 3750 validated image filenames.\n",
      "Epoch 1/5\n",
      "118/117 [==============================] - 43s 368ms/step - loss: 0.5549 - mean_squared_error: 0.5549 - val_loss: 1.6357 - val_mean_squared_error: 1.6357\n",
      "Epoch 2/5\n",
      "118/117 [==============================] - 41s 347ms/step - loss: 0.5441 - mean_squared_error: 0.5441 - val_loss: 2.2138 - val_mean_squared_error: 2.2138\n",
      "Epoch 3/5\n",
      "118/117 [==============================] - 41s 347ms/step - loss: 0.5306 - mean_squared_error: 0.5306 - val_loss: 2.1595 - val_mean_squared_error: 2.1595\n",
      "Epoch 4/5\n",
      "118/117 [==============================] - 41s 351ms/step - loss: 0.5099 - mean_squared_error: 0.5099 - val_loss: 2.1042 - val_mean_squared_error: 2.1042\n",
      "Epoch 5/5\n",
      "118/117 [==============================] - 41s 347ms/step - loss: 0.5121 - mean_squared_error: 0.5121 - val_loss: 2.3277 - val_mean_squared_error: 2.3277\n",
      "Found 3750 validated image filenames.\n",
      "Epoch 1/5\n",
      "118/117 [==============================] - 43s 367ms/step - loss: 0.5216 - mean_squared_error: 0.5216 - val_loss: 2.2310 - val_mean_squared_error: 2.2310\n",
      "Epoch 2/5\n",
      "118/117 [==============================] - 41s 345ms/step - loss: 0.5323 - mean_squared_error: 0.5323 - val_loss: 2.4333 - val_mean_squared_error: 2.4333\n",
      "Epoch 3/5\n",
      "118/117 [==============================] - 41s 347ms/step - loss: 0.4929 - mean_squared_error: 0.4929 - val_loss: 1.9721 - val_mean_squared_error: 1.9721\n",
      "Epoch 4/5\n",
      "118/117 [==============================] - 41s 346ms/step - loss: 0.4754 - mean_squared_error: 0.4754 - val_loss: 2.7858 - val_mean_squared_error: 2.7858\n",
      "Epoch 5/5\n",
      "118/117 [==============================] - 41s 346ms/step - loss: 0.4914 - mean_squared_error: 0.4914 - val_loss: 2.5120 - val_mean_squared_error: 2.5120\n"
     ]
    }
   ],
   "source": [
    "#set up cross-validation\n",
    "kfold = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "trustworthiness_val_rmse = []\n",
    "confidence_val_rmse = []\n",
    "sexual_interest_val_rmse = []\n",
    "intelligence_val_rmse = []\n",
    "happiness_val_rmse = []\n",
    "trustworthiness_cors = []\n",
    "confidence_cors = []\n",
    "sexual_interest_cors = []\n",
    "intelligence_cors = []\n",
    "happiness_cors = []\n",
    "\n",
    "for train, test in kfold.split(labels):\n",
    "    \n",
    "    \n",
    "    history = model.fit_generator(datagen.flow_from_dataframe(dataframe=labels.iloc[train], directory=\"../input/resized-images/resized_images/resized_images/\", \n",
    "                                                  x_col=\"player.photo\", y_col=target, has_ext=True, \n",
    "                                                  class_mode=\"other\", target_size=(224, 224), \n",
    "                                                  batch_size=batchsize), steps_per_epoch=labels.iloc[train].shape[0] / batchsize, epochs=nr_epochs, validation_data=(X[test], labels.iloc[test][target])) \n",
    "    \n",
    "    ytrue = labels.iloc[test][target]\n",
    "    ypreds = model.predict(X[test])\n",
    "    \n",
    "    trustworthiness_cor = np.corrcoef(ypreds[:,0], ytrue.iloc[:,0])\n",
    "    trustworthiness_val_mse = mean_squared_error(ypreds[:,0], ytrue.iloc[:,0])\n",
    "    trustworthiness_rmse = sqrt(trustworthiness_val_mse)\n",
    "    trustworthiness_val_rmse.append(trustworthiness_rmse)\n",
    "\n",
    "    sexual_interest_cor = np.corrcoef(ypreds[:,1], ytrue.iloc[:,1])\n",
    "    sexual_interest_val_mse = mean_squared_error(ypreds[:,1], ytrue.iloc[:,1])\n",
    "    sexual_interest_rmse = sqrt(sexual_interest_val_mse)\n",
    "    sexual_interest_val_rmse.append(sexual_interest_rmse)\n",
    "\n",
    "    intelligence_cor = np.corrcoef(ypreds[:,2], ytrue.iloc[:,2])\n",
    "    intelligence_val_mse = mean_squared_error(ypreds[:,2], ytrue.iloc[:,2])\n",
    "    intelligence_rmse = sqrt(intelligence_val_mse)\n",
    "    intelligence_val_rmse.append(intelligence_rmse)\n",
    "\n",
    "    confidence_cor = np.corrcoef(ypreds[:,3], ytrue.iloc[:,3])\n",
    "    confidence_val_mse = mean_squared_error(ypreds[:,3], ytrue.iloc[:,3])\n",
    "    confidence_rmse = sqrt(confidence_val_mse)\n",
    "    confidence_val_rmse.append(confidence_rmse)\n",
    "\n",
    "    happiness_cor = np.corrcoef(ypreds[:,4], ytrue.iloc[:,4])\n",
    "    happiness_val_mse = mean_squared_error(ypreds[:,4], ytrue.iloc[:,4])\n",
    "    happiness_rmse = sqrt(happiness_val_mse)\n",
    "    happiness_val_rmse.append(happiness_rmse)\n",
    "    \n",
    "    confidence_cors.append(confidence_cor[0,1])\n",
    "    sexual_interest_cors.append(sexual_interest_cor[0,1])\n",
    "    happiness_cors.append(happiness_cor[0,1])\n",
    "    intelligence_cors.append(intelligence_cor[0,1])\n",
    "    trustworthiness_cors.append(trustworthiness_cor[0,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "dict_keys(['val_loss', 'val_mean_squared_error', 'loss', 'mean_squared_error'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# history = model\n",
    "print(\"\\n\")\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33988067323483123, 0.3989847263216669, 0.382501339965939, 0.4475143920979739]\n"
     ]
    }
   ],
   "source": [
    "print(confidence_cors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission = pd.DataFrame({'sexual': np.median(sexual_interest_cors),'confident': np.median(confidence_cors),'happy': np.median(happiness_cors), 'intel': np.median(intelligence_cors), 'trust': np.median(trustworthiness_cors)}, index=[0])\n",
    "my_submission.to_csv(str(lrr) + str(lrr) + str(unfreeze) + '_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
